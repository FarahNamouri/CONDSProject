{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some of the packages and libraries used\n",
    "* **math:** offers mathematical functions, for example: logarithm.\n",
    "* **mmh3:** uses the MurmurHash3 hashing algorithm to generate the hash values.\n",
    "* **hashlib:** provides secure hash algorithms, such as: SHA256.\n",
    "* **bitarray:** offers arrays of bits that can be used at bit level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bloom Filter class\n",
    "\n",
    "We first created a bloom filter class that has the following methods: \n",
    "\n",
    "* **calc_length:** calculates the size of a bloom filter.\n",
    "* **optimum_hashes:** ccalculates the optimal number of hash functions to use.\n",
    "* **insert:** adds elements in the bloom filter.\n",
    "* **verify:** to check the existence of an elements in the bloom filter.\n",
    "* **compute_fpr:** calculates the false positive rate of a bloom filter.\n",
    "* **plot_fpr:** plots the the false positive rate of a bloom filter as a function of the number of words inserted in the bloom filter.\n",
    "* **compute_cr:** calculates the compression rate of a bloom filter.\n",
    "* **plot_cr:** plots the compression rate of a bloom filter as a function of the expected number of elements and the rate of false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The insert function performance:\n",
    "The relationship between the insertion time and the number of inputs should be linear. When the input size increases, the time of insertion should also increase.\n",
    "\n",
    "# The verify function performance:\n",
    "This relationship between the verification time and the number of elements should also be linear. As the number of elements increases, the seach time also increases.\n",
    "\n",
    "\n",
    "\n",
    "We tested both evaluation performances using benchmarking (in HPC), which is useful for large datasets that can't be computed on regular computers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7\n",
    "Check how the false positive rate changes as a function of the number of words inserted in the Bloom filter. Also check this if the number of words exceeds the expected number of words for which the Bloom filter was designed.\n",
    "\n",
    "In this question, we are interested in calculating the false positive rate which is calculated using the following formula:\n",
    "$$\n",
    "fpr = \\left(1 - \\left(1 - \\frac{1}{m}\\right)^{kn}\\right)^k\n",
    "$$\n",
    "\n",
    "explanation:\n",
    "- fpr: False Positive Rate \n",
    "- m: size of the bloom filter (in bits)\n",
    "- k: number of hash functions used\n",
    "- n: number of elements that have been inserted into the Bloom filter (input number)\n",
    "\n",
    "Obviously, the FPR doesn't have a linear relationship with the number of inputs. It's an exponential relationship. Also, the FPR increases in an exponential way when the number of elements increase.\n",
    "This was seen in the plot created in question 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8\n",
    "Check the compression rate of a Bloom filter as a function of the expected number of and the rate of false positives.\n",
    "\n",
    "The compression rate is the ratio of the traditional storage size of the data by the actual size of the bloom filter.\n",
    "\n",
    "The idea in this question is that when we fix the array size, the compression rate decreases as the number of elements increases until all bits are used and the compression rate will almost be zero. The plot therefore, should be somewhat a decay exponential, as illustrated in the output of our code."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
